{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import sys\n",
    "import uuid\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.autograd import Variable\n",
    "from ax import *\n",
    "from ax.plot.scatter import plot_fitted\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.utils.stats.statstools import agresti_coull_sem\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting reproducability\n",
    "manualSeed = 158138\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# if you are suing GPU\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "FIRST_ARM=5\n",
    "IT_ARM=3\n",
    "ITRATION=5\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your dataset class\n",
    "class MJDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.wfin = np.load(\"train_input.npy\")\n",
    "        self.wfout = np.load(\"train_labels.npy\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(np.concatenate([self.wfin, self.wfout],axis=0))\n",
    "        self.wfin = self.scaler.transform(self.wfin)\n",
    "        self.wfout = self.scaler.transform(self.wfout)\n",
    "        self.wfsize = self.wfin.shape[-1]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wfout)\n",
    "    # @torchsnooper.snoop()\n",
    "    def __getitem__(self, idx):\n",
    "        return self.wfin, self.wfout[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,conv1,conv2,conv3,conv4,conv5,fc1,fc2,fc3,fc4):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, conv1, kernel_size=5, padding=2), # 1 * 2016 -> conv1 * 2016\n",
    "            nn.BatchNorm1d(conv1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)) # conv1 * 2016 -> conv1 * 1008\n",
    "        self.layer2  = nn.Sequential(\n",
    "            nn.Conv1d(conv1, conv2, kernel_size=5, padding=2), # conv1 * 1008 -> conv2 * 1008\n",
    "            nn.BatchNorm1d(conv2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)) # conv2 * 1008 -> conv2 * 504\n",
    "        self.layer3  = nn.Sequential(\n",
    "            nn.Conv1d(conv2, conv3, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv3), # conv2 * 504 -> conv3 * 504\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)) # conv3 * 504 -> conv3 * 252\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(conv3, conv4, kernel_size=5, padding=2), # conv3 * 252 -> conv4 * 252\n",
    "            nn.BatchNorm1d(conv4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)) # conv4 * 252 -> conv4 * 126\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv1d(conv4, conv5, kernel_size=5, padding=2), # conv4 * 126 -> conv5 * 126\n",
    "            nn.BatchNorm1d(conv5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)) # conv5 * 126 -> conv5 * 63\n",
    "        self.fc1 = nn.Linear(63*conv5, fc1)\n",
    "        self.fc2 = nn.Linear(fc1,fc2)\n",
    "        self.fc3 = nn.Linear(fc2,fc3)\n",
    "        self.fc4 = nn.Linear(fc3,fc4)\n",
    "        self.fc5 = nn.Linear(fc4,1)\n",
    "    \n",
    "    #@torchsnooper.snoop()\n",
    "    # defines how an input tensor flows thru the network\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        #for i in range(0,out.size()[0]):\n",
    "            #scored = out[i].item()\n",
    "            #if (scored > 0.7):\n",
    "                #plt.plot(range(0,len(x[i].numpy()[0])),x[i].numpy()[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params):\n",
    "    #Reconstructing original data\n",
    "    train_set  = torch.Tensor(np.load(\"/project/projectdirs/majorana/users/stew314/train_set.npy\"))\n",
    "    train_labels = torch.Tensor(np.load(\"/project/projectdirs/majorana/users/stew314/train_labels.npy\"))\n",
    "    dataset = torch.utils.data.TensorDataset(train_set,train_labels)\n",
    "\n",
    "    print(params)\n",
    "\n",
    "    NUM_EPOCHS = params[\"num_epoch\"]\n",
    "    batch_size= params[\"batch_size\"]\n",
    "    LEARNING_RATE =10**(params[\"learning_rate\"])\n",
    "    ntokens = params[\"ntokens\"]\n",
    "    emsize = 2*params[\"nhead\"] * params[\"emmultiplier\"]\n",
    "    nhid = params[\"nhid\"]\n",
    "    nlayers = params[\"nlayers\"]\n",
    "    nhead = 2*params[\"nhead\"]\n",
    "    dropout = params[\"dropout\"]\n",
    "    conv1 = params[\"conv1\"]\n",
    "    conv2 = params[\"conv2\"]\n",
    "    conv3 = params[\"conv3\"]\n",
    "    conv4 = params[\"conv4\"]\n",
    "    conv5 = params[\"conv5\"]\n",
    "    fc1 = params[\"fc1\"]\n",
    "    fc2 = params[\"fc2\"]\n",
    "    fc3 = params[\"fc3\"]\n",
    "    fc4 = params[\"fc4\"]\n",
    "    optimizer_choice = params[\"optimizer\"]\n",
    "\n",
    "    validation_split = .3\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42222\n",
    "    indices = np.arange(len(dataset))\n",
    "\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    split = int(validation_split*len(dataset))\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = data_utils.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler,  drop_last=True)\n",
    "\n",
    "    classifier = Model(conv1,conv2,conv3,conv4,conv5,fc1,fc2,fc3,fc4)\n",
    "    print(\"#params\", sum(x.numel() for x in classifier.parameters()))\n",
    "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    classifier.to(DEVICE)\n",
    "\n",
    "    # criterion = UnsupervisedCELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    criterion = criterion.to(DEVICE)\n",
    "\n",
    "#     optimizer = torch.optim.RMSprop(\n",
    "#         classifier.parameters(),\n",
    "#         momentum=params[\"momentum\"],\n",
    "#         lr=LEARNING_RATE)\n",
    "    if(optimizer_choice == \"Adam\"):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=LEARNING_RATE)\n",
    "    elif(optimizer_choice == \"SGD\"):\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad,\n",
    "            classifier.parameters()),\n",
    "            lr=LEARNING_RATE)\n",
    "    elif(optimizer_choice == \"RMSprop\"):\n",
    "        optimizer = torch.optim.RMSprop(classifier.parameters(),\n",
    "            lr=LEARNING_RATE)\n",
    "        \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-4,cycle_momentum=False)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5,threshold=0.0002,threshold_mode='abs')\n",
    "    loss_array = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for i, (wf,wfsmooth) in enumerate(train_loader):\n",
    "            classifier.train()\n",
    "            wf = wf.to(DEVICE).float()\n",
    "            wfsmooth = wfsmooth.to(DEVICE).float()\n",
    "\n",
    "            outputs  = classifier(wf)\n",
    "            loss = criterion(outputs.squeeze(-1),wfsmooth)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()        # update parameters of net\n",
    "            optimizer.zero_grad()   # reset gradient\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if (i % 100 == 0):\n",
    "                print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "                    epoch+1, NUM_EPOCHS, i+1, len(train_loader),\n",
    "                    loss.item(), end=\"\"))\n",
    "        # scheduler.step(loss)\n",
    "        if epoch>NUM_EPOCHS-5:\n",
    "            loss_array.append(loss.item())\n",
    "    loss_array = np.array(loss_array)\n",
    "    print(\"Average Loss: %.4f\"%(np.average(loss_array[loss_array!=np.max(loss_array)])))\n",
    "    return -np.average(loss_array[loss_array!=np.max(loss_array)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Parameters\n",
    "p1 = FixedParameter(name=\"num_epoch\", value=3, parameter_type=ParameterType.INT)\n",
    "p2 = RangeParameter(name=\"batch_size\", lower=8, upper=20, parameter_type=ParameterType.INT)\n",
    "#p3  = RangeParameter(name=\"learning_rate\", lower=-5.0, upper=-3.0, parameter_type=ParameterType.FLOAT)\n",
    "p3  = FixedParameter(name=\"learning_rate\", value=-4.0, parameter_type=ParameterType.FLOAT)\n",
    "p4 = RangeParameter(name=\"ntokens\", lower=3, upper=12, parameter_type=ParameterType.INT)\n",
    "p5 = RangeParameter(name=\"emmultiplier\", lower=3, upper=10, parameter_type=ParameterType.INT)\n",
    "p6 = RangeParameter(name=\"nhid\", lower=64, upper=196, parameter_type=ParameterType.INT)\n",
    "p7 = RangeParameter(name=\"nlayers\", lower=2, upper=20, parameter_type=ParameterType.INT)\n",
    "p8 = RangeParameter(name=\"nhead\", lower=1,upper=15, parameter_type=ParameterType.INT)\n",
    "p9  = RangeParameter(name=\"dropout\", lower=0.0, upper=0.7, parameter_type=ParameterType.FLOAT)\n",
    "p10 =  ChoiceParameter(name=\"optimizer\", values=[\"Adam\",\"SGD\",\"RMSprop\"], parameter_type=ParameterType.STRING)\n",
    "p11 = ChoiceParameter(name=\"conv1\", values=[16,32,64], parameter_type=ParameterType.INT)\n",
    "p12 = ChoiceParameter(name=\"conv2\", values=[32,64,128], parameter_type=ParameterType.INT)\n",
    "p13 = ChoiceParameter(name=\"conv3\", values=[64,128,256], parameter_type=ParameterType.INT)\n",
    "p14 = ChoiceParameter(name=\"conv4\", values=[128,256,512], parameter_type=ParameterType.INT)\n",
    "p15 = ChoiceParameter(name=\"conv5\", values=[256,512,1024], parameter_type=ParameterType.INT)\n",
    "p16 = ChoiceParameter(name=\"fc1\", values=[512,256,128], parameter_type=ParameterType.INT)\n",
    "p17 = ChoiceParameter(name=\"fc2\", values=[256,128,64], parameter_type=ParameterType.INT)\n",
    "p18 = ChoiceParameter(name=\"fc3\", values=[128,64,32], parameter_type=ParameterType.INT)\n",
    "p19 = ChoiceParameter(name=\"fc4\", values=[64,32,16], parameter_type=ParameterType.INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cd:\n",
    "    '''\n",
    "    Context manager for changing the current working directory\n",
    "    '''\n",
    "    def __init__(self, newPath):\n",
    "        self.newPath = newPath\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.savedPath = os.getcwd()\n",
    "        os.chdir(self.newPath)\n",
    "\n",
    "    def __exit__(self, etype, value, traceback):\n",
    "        os.chdir(self.savedPath)\n",
    "\n",
    "class BoothMetric(Metric):\n",
    "    def fetch_trial_data(self, trial):  \n",
    "        records = []\n",
    "        auc_result = trial.run_metadata[\"auc\"]\n",
    "        index = 0\n",
    "        for arm_name, arm in trial.arms_by_name.items():\n",
    "            params = arm.parameters\n",
    "            records.append({\n",
    "                \"arm_name\": arm_name,\n",
    "                \"metric_name\": self.name,\n",
    "                \"mean\": auc_result[index],\n",
    "                \"sem\": 0.0,\n",
    "                \"trial_index\": trial.index\n",
    "            })\n",
    "            index += 1\n",
    "        return Data(df=pd.DataFrame.from_records(records))\n",
    "        \n",
    "class MyRunner(Runner):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        nothing\n",
    "        '''\n",
    "\n",
    "    def run(self, trial):\n",
    "        arm_result = []\n",
    "        for arm_name, arm in trial.arms_by_name.items():\n",
    "            params = arm.parameters\n",
    "            auc = train(params)\n",
    "            arm_result.append(float(auc))\n",
    "        return {\"name\": str(trial.index), \"auc\": arm_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 12, 'ntokens': 9, 'emmultiplier': 7, 'nhid': 154, 'nlayers': 11, 'nhead': 15, 'dropout': 0.03591483626514673, 'optimizer': 'Adam', 'conv1': 64, 'conv2': 128, 'conv3': 128, 'conv4': 512, 'conv5': 512, 'fc1': 256, 'fc2': 128, 'fc3': 32, 'fc4': 32, 'num_epoch': 3, 'learning_rate': -4.0}\n",
      "#params 10061537\n",
      "Epoch [1/3], Iter [1/4666] Loss: 0.5002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_space = SearchSpace(\n",
    "    parameters=[p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19],\n",
    ")#make sure to all all defined parameters here\n",
    "\n",
    "experiment = Experiment(\n",
    "    name=\"hyper_parameter_optimization\",\n",
    "    search_space=search_space,\n",
    ")\n",
    "optimization_config = OptimizationConfig(\n",
    "    objective = Objective(\n",
    "        metric=BoothMetric(name=\"booth\"), \n",
    "        minimize=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "experiment.optimization_config = optimization_config\n",
    "\n",
    "sobol = Models.SOBOL(search_space=experiment.search_space)\n",
    "generator_run = sobol.gen(FIRST_ARM)\n",
    "\n",
    "experiment.runner = MyRunner()\n",
    "experiment.new_batch_trial(generator_run=generator_run)\n",
    "\n",
    "experiment.trials[0].run().mark_completed()\n",
    "data = experiment.fetch_data()\n",
    "\n",
    "for i in range(1, ITRATION):\n",
    "\n",
    "    data = experiment.fetch_data()\n",
    "    gpei = Models.GPEI(experiment=experiment, data=data)\n",
    "    generator_run = gpei.gen(IT_ARM)\n",
    "    experiment.new_batch_trial(generator_run=generator_run)\n",
    "    experiment.trials[i].run().mark_completed()\n",
    "    data = experiment.fetch_data()\n",
    "    df = data.df\n",
    "    best_arm_name = df.arm_name[df['mean'] == df['mean'].max()].values[0]\n",
    "    best_arm = experiment.arms_by_name[best_arm_name]\n",
    "    print(best_arm)\n",
    "    json_field = best_arm.parameters\n",
    "    json_field[\"improvement\"] = df['mean'].max()\n",
    "    with open('data_alpha_v2.json', 'w') as fp:\n",
    "        json.dump(json_field, fp)\n",
    "    df.to_json(r'arms_alpha_v2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
